# LFactory ì—°êµ¬ ê³„íš (ìˆ˜ì •íŒ)
## LLM-Guided Manufacturing Anomaly Detection & Interpretation

**ì‘ì„±ì¼**: 2025-11-25  
**ìƒíƒœ**: Phase 1 ì™„ë£Œ â†’ Phase 2-3 ì§„í–‰ ì¤‘  
**í•µì‹¬ ëª©í‘œ**: ì œì¡° ê³µì • ë°ì´í„°ì—ì„œ ML ì´ìƒ íƒì§€ + LLM í•´ì„ + LLM ê¸°ë°˜ ìµœì í™”  

---

## ğŸ¯ ì—°êµ¬ì˜ ì§„ì§œ ëª©ì  (Real Objective)

```
ì œì¡° ê³µì • ë°ì´í„°
    â†“ (Phase 1: Detect)
ML ì´ìƒ íƒì§€ (IsolationForest, LSTM-AE, ...)
    â†“ (Phase 2: Explain)
LLMì´ 3ê°€ì§€ í•´ì„:
  1. ì´ìƒì¹˜ ìì²´ í•´ì„ (ì™œ ì´ìƒì¸ê°€?)
  2. ML ëª¨ë¸ í•´ì„ (ëª¨ë¸ì´ ì™œ ì´ë ‡ê²Œ íŒë‹¨í–ˆë‚˜?)
  3. ë„ë©”ì¸ ì§€ì‹ ì—°ê²° (ì œì¡° ê³µì • ê´€ì ì—ì„œ ì˜ë¯¸ëŠ”?)
    â†“ (Phase 3: Optimize)
LLMì´ ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ML íŒŒë¼ë¯¸í„° ì œì•ˆ
    â†“
ê°œì„ ëœ ML íƒì§€ê¸°
```

### í•µì‹¬ ì°¨ë³„ì 
- âŒ **ë‹¨ìˆœ ì´ìƒ íƒì§€**: ê¸°ì¡´ ì—°êµ¬ ë§ìŒ
- âŒ **MLë§Œ**: í•´ì„ ë¶ˆê°€ëŠ¥
- âœ… **ML + LLM í†µí•©**: íƒì§€ + í•´ì„ + ìµœì í™” â† **ìš°ë¦¬ ì—°êµ¬!**

---

## ğŸ“Š í˜„ì¬ ì™„ë£Œ ìƒíƒœ (2025-11-25)

### âœ… Phase 1: ML ì´ìƒ íƒì§€ (100% ì™„ë£Œ)
- [x] 6ê°€ì§€ detector êµ¬í˜„: Rule, kNN, IsolationForest, LSTM-AE, Hybrid, SpecCNN
- [x] 480 runs ì‹¤í—˜ (4 datasets Ã— 6 detectors Ã— 20 seeds)
- [x] í†µê³„ ê²€ì¦ (Wilcoxon test, Bootstrap CI, Correlation)
- [x] **í•µì‹¬ ë°œê²¬**:
  - SKAB: LSTM-AE ìµœê³  (F1=0.087, AUC-PR=0.338)
  - SMD: IsolationForest ìµœê³  (F1=0.458, AUC-PR=0.543)
  - Point-wise Recall â†” Event-wise Precision ê°•í•œ ìƒê´€ (r=0.799)
- [x] ì¢…í•© ë³´ê³ ì„œ: `COMPREHENSIVE_EXPERIMENT_REPORT.md` (35í˜ì´ì§€)

### âŒ Phase 2-3: LLM í†µí•© (0% ì™„ë£Œ - ìµœìš°ì„ !)
- [ ] LLM ì´ìƒì¹˜ í•´ì„
- [ ] **LLM ML ëª¨ë¸ í•´ì„** â† **ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­!**
- [ ] LLM ë„ë©”ì¸ ìµœì í™”

---

## ğŸ”¥ Phase 2: LLM í•´ì„ ì‹œìŠ¤í…œ (Week 1-2, ìµœìš°ì„ !)

### ëª©í‘œ
ì œì¡° ê³µì • ì´ìƒì„ LLMì´ **3ê°€ì§€ ê´€ì ì—ì„œ í•´ì„**

### 2.1 ì´ìƒì¹˜ í•´ì„ (Anomaly Explanation)

**ì…ë ¥**:
```python
anomaly = {
    "timestamp": 1234,
    "dataset": "SKAB",
    "sensor_values": {"P1": 5.2, "TE1": 85.3, "F1": 0.3},
    "anomaly_score": 0.89,
    "detector": "IsolationForest",
    "threshold": 0.5
}
```

**LLM ì¶œë ¥**:
```
âš ï¸ ì´ìƒ íƒì§€ (SKAB Valve System, t=1234)

**ë¬´ì—‡ì´ ë°œìƒí–ˆë‚˜**:
- ì••ë ¥(P1): 5.2 bar â¬†ï¸ (ì •ìƒ ë²”ìœ„: 3-4 bar)
- ì˜¨ë„(TE1): 85.3Â°C â¬†ï¸ (ì •ìƒ ë²”ìœ„: 60-70Â°C)
- ìœ ëŸ‰(F1): 0.3 L/min â¬‡ï¸ (ì •ìƒ ë²”ìœ„: 1.5-2.0 L/min)

**ì™œ ì´ìƒì¸ê°€**:
IsolationForest ìŠ¤ì½”ì–´ 0.89ëŠ” ì´ (ì••ë ¥â†‘, ì˜¨ë„â†‘, ìœ ëŸ‰â†“) ì¡°í•©ì´
ì •ìƒ ë™ì‘ íŒ¨í„´ê³¼ ë§¤ìš° ë‹¤ë¦„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

**ì œì¡° ê³µì • ê´€ì **:
ë°¸ë¸Œ ë§‰í˜ì˜ ì „í˜•ì  ì‹ í˜¸ - ì••ë ¥ ì¦ê°€, ìœ ëŸ‰ ê°ì†Œ, ë§ˆì°°ì—´ ë°œìƒ

**ê°€ëŠ¥í•œ ì›ì¸**:
1. ë°¸ë¸Œ ë‚´ë¶€ ì´ë¬¼ì§ˆ ë˜ëŠ” ìŠ¤ì¼€ì¼ ì¶•ì  (ê°€ëŠ¥ì„± 70%)
2. ë°¸ë¸Œ ì‹œíŠ¸ ì†ìƒìœ¼ë¡œ ì¸í•œ ë¶ˆì™„ì „ ê°œí (ê°€ëŠ¥ì„± 20%)
3. ì„¼ì„œ ì˜¤ë¥˜ (ê°€ëŠ¥ì„± 10%)

**ê¶Œì¥ ì¡°ì¹˜**:
1. ì¦‰ì‹œ: ë°¸ë¸Œ ì ê²€ ìŠ¤ì¼€ì¤„ (ë‹¤ìŒ ì •ì§€ ì‹œ)
2. ëª¨ë‹ˆí„°ë§ ê°•í™”: ì••ë ¥/ì˜¨ë„ 1ë¶„ ê°„ê²© ì²´í¬
3. ì˜ˆë¹„ ë°¸ë¸Œ ì¤€ë¹„
4. ì„¼ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê¸°ë¡ í™•ì¸
```

**êµ¬í˜„**:
```python
# experiments/llm_anomaly_explainer.py

import openai

OPENAI_API_KEY = "sk-proj-..." # .claude/claude.mdì—ì„œ ê°€ì ¸ì˜´

def explain_anomaly(anomaly, domain_knowledge):
    """LLMì´ ì´ìƒì¹˜ë¥¼ í•´ì„"""

    prompt = f"""
You are an expert in manufacturing anomaly analysis.

Dataset: {anomaly['dataset']}
Domain: {domain_knowledge['domain_description']}
Normal ranges: {domain_knowledge['sensor_ranges']}

Detected anomaly:
- Time: {anomaly['timestamp']}
- Sensors: {anomaly['sensor_values']}
- Anomaly score: {anomaly['anomaly_score']}
- Detector: {anomaly['detector']}

Explain this anomaly in 4 parts:
1. What happened (sensor deviations)
2. Why it's anomalous (statistical perspective)
3. Manufacturing perspective (domain meaning)
4. Possible causes with probability estimates
5. Recommended actions

Be specific, actionable, and consider manufacturing domain knowledge.
"""

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a manufacturing anomaly expert."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content
```

### 2.2 ML ëª¨ë¸ í•´ì„ (Model Explanation) â­ **í•µì‹¬!**

**ëª©í‘œ**: LLMì´ **ML ëª¨ë¸ì˜ íŒë‹¨ ê³¼ì •ì„ í•´ì„**

**ì‹œë‚˜ë¦¬ì˜¤**:
```
"IsolationForestê°€ ì™œ ì´ ì ì„ anomalyë¡œ íŒë‹¨í–ˆëŠ”ê°€?"
"ì–´ë–¤ featureê°€ ê°€ì¥ ì¤‘ìš”í–ˆëŠ”ê°€?"
"Decision boundaryëŠ” ì–´ë–»ê²Œ ìƒê²¼ëŠ”ê°€?"
```

**êµ¬í˜„ ë°©ë²•**:

#### 2.2.1 Feature Importance í•´ì„
```python
# IsolationForest feature importance ì¶”ì¶œ
from sklearn.inspection import permutation_importance

# ëª¨ë¸ í•™ìŠµ
model = IsolationForest(...)
model.fit(X_train)

# Feature importance
result = permutation_importance(model, X_test, scoring='roc_auc')
feature_importance = dict(zip(feature_names, result.importances_mean))

# LLMì—ê²Œ í•´ì„ ìš”ì²­
llm_explanation = explain_feature_importance(
    model="IsolationForest",
    features=feature_importance,
    domain="SKAB valve system",
    anomaly_point=anomaly
)
```

**LLM ì¶œë ¥ ì˜ˆì‹œ**:
```
**IsolationForest ëª¨ë¸ í•´ì„**

ì´ ëª¨ë¸ì´ anomalyë¡œ íŒë‹¨í•œ ì´ìœ :

**ê°€ì¥ ì¤‘ìš”í•œ Feature (Top 3)**:
1. **ìœ ëŸ‰(F1) ì°¨ì´**: ì¤‘ìš”ë„ 0.42
   - ì •ìƒ: 1.8 L/min, ì´ìƒ: 0.3 L/min
   - 83% ê°ì†Œ â†’ ëª¨ë¸ì´ ê°€ì¥ ê°•í•˜ê²Œ ë°˜ì‘

2. **ì••ë ¥(P1) ì¦ê°€**: ì¤‘ìš”ë„ 0.31
   - ì •ìƒ: 3.5 bar, ì´ìƒ: 5.2 bar
   - 49% ì¦ê°€ â†’ ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ì‹ í˜¸

3. **ì˜¨ë„(TE1) ì¦ê°€**: ì¤‘ìš”ë„ 0.18
   - ì •ìƒ: 65Â°C, ì´ìƒ: 85.3Â°C
   - 31% ì¦ê°€ â†’ ë³´ì¡° ì‹ í˜¸

**ëª¨ë¸ì˜ íŒë‹¨ ë…¼ë¦¬**:
IsolationForestëŠ” "ìœ ëŸ‰ ê¸‰ê° + ì••ë ¥ ê¸‰ì¦" ì¡°í•©ì´
ì •ìƒ ë°ì´í„°ì—ì„œ ê±°ì˜ ê´€ì°°ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

ì´ ì¡°í•©ì€ ì „ì²´ í•™ìŠµ ë°ì´í„°ì˜ 0.5%ì—ì„œë§Œ ë°œìƒ â†’ Isolation ì‰¬ì›€

**ì‹ ë¢°ë„**: ë†’ìŒ (3ê°œ feature ëª¨ë‘ ì¼ê´€ëœ ì‹ í˜¸)
```

#### 2.2.2 SHAP/LIME í•´ì„ + LLM
```python
import shap

# SHAP values ê³„ì‚°
explainer = shap.Explainer(model, X_train)
shap_values = explainer(anomaly_point)

# LLMì´ SHAP valuesë¥¼ ìì—°ì–´ë¡œ ì„¤ëª…
llm_shap_explanation = explain_shap_values(
    shap_values=shap_values,
    feature_names=feature_names,
    domain="manufacturing"
)
```

**LLM ì¶œë ¥ ì˜ˆì‹œ**:
```
**SHAP ë¶„ì„ ê²°ê³¼ (ëª¨ë¸ì˜ ì„¸ë¶€ íŒë‹¨)**

ì´ anomaly score 0.89ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤:

Base score (í‰ê· ):        0.15
+ F1 contribution:       +0.35 (ìœ ëŸ‰ ê¸‰ê°ì˜ ì˜í–¥)
+ P1 contribution:       +0.28 (ì••ë ¥ ì¦ê°€ì˜ ì˜í–¥)
+ TE1 contribution:      +0.11 (ì˜¨ë„ ì¦ê°€ì˜ ì˜í–¥)
= Final score:            0.89

**í•´ì„**:
- ìœ ëŸ‰ ê°ì†Œê°€ ê°€ì¥ í° ì˜í–¥ (ì „ì²´ scoreì˜ 47%)
- ì••ë ¥ ì¦ê°€ê°€ ë‘ ë²ˆì§¸ (ì „ì²´ scoreì˜ 37%)
- ì˜¨ë„ëŠ” ë³´ì¡°ì  ì—­í•  (ì „ì²´ scoreì˜ 15%)

ë§Œì•½ ìœ ëŸ‰ì´ ì •ìƒì´ì—ˆë‹¤ë©´ scoreëŠ” 0.54 ì •ë„ì˜€ì„ ê²ƒ (threshold 0.5 ì´ˆê³¼í•˜ì§€ë§Œ ëœ í™•ì‹¤)
```

#### 2.2.3 Decision Boundary ì‹œê°í™” + LLM ì„¤ëª…
```python
# 2D projectionìœ¼ë¡œ decision boundary ì‹œê°í™”
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_2d = pca.fit_transform(X)
anomaly_2d = pca.transform(anomaly_point)

# Plot decision boundary
plot_decision_boundary(model, X_2d, anomaly_2d)

# LLMì´ ì‹œê°í™”ë¥¼ ì„¤ëª…
llm_boundary_explanation = explain_decision_boundary(
    anomaly_position=anomaly_2d,
    normal_region_center=normal_center_2d,
    distance_to_boundary=distance
)
```

**LLM ì¶œë ¥ ì˜ˆì‹œ**:
```
**Decision Boundary ë¶„ì„**

ì´ìƒ ì ì˜ ìœ„ì¹˜:
- PCA 2D ê³µê°„ì—ì„œ ì¢Œí‘œ: (3.2, -1.8)
- ì •ìƒ ì˜ì—­ ì¤‘ì‹¬ì—ì„œ ê±°ë¦¬: 4.5 (í‘œì¤€í¸ì°¨ ë‹¨ìœ„)

**ì‹œê°ì  ì„¤ëª…**:
ì •ìƒ ë°ì´í„°ëŠ” ì›ì  (0,0) ì£¼ë³€ì— ë°€ì§‘ëœ íƒ€ì›í˜• ë¶„í¬.
ì´ anomalyëŠ” ì •ìƒ ì˜ì—­ì—ì„œ ì˜¤ë¥¸ìª½ ì•„ë˜ ë°©í–¥ìœ¼ë¡œ
í¬ê²Œ ë²—ì–´ë‚˜ ìˆìŠµë‹ˆë‹¤ (4.5Ïƒ).

ì´ëŠ” "ë†’ì€ ì••ë ¥ + ë‚®ì€ ìœ ëŸ‰" ì¡°í•©ì´
ì •ìƒ ë™ì‘ íŒ¨í„´ê³¼ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¦„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ë¡€**:
í•™ìŠµ ë°ì´í„°ì—ì„œ ì´ ì˜ì—­ì— ìˆë˜ ì ë“¤ì€:
- 80%ê°€ "ë°¸ë¸Œ ë§‰í˜" ë¼ë²¨
- 15%ê°€ "ì„¼ì„œ ì˜¤ë¥˜" ë¼ë²¨
- 5%ê°€ ì˜¤íƒì§€
```

### 2.3 ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•

**ëª©í‘œ**: LLMì´ ì°¸ì¡°í•  ì œì¡° ê³µì • ì§€ì‹ ì‘ì„±

**íŒŒì¼**: `experiments/knowledge_base/SKAB_valve_system.md`

```markdown
# SKAB Valve System Domain Knowledge

## System Description
Industrial valve monitoring system with 8 sensors tracking pressure, temperature, and flow.

## Sensors
1. **P1**: Inlet pressure (bar)
   - Normal range: 3-4 bar
   - Critical: >6 bar (safety valve trigger)

2. **P2**: Outlet pressure (bar)
   - Normal range: 2-3 bar
   - Should be lower than P1

3. **TE1**: Inlet temperature (Â°C)
   - Normal range: 60-70Â°C
   - Critical: >90Â°C (overheating risk)

4. **TE2**: Outlet temperature (Â°C)
   - Normal range: 55-65Â°C

5. **F1**: Flow rate (L/min)
   - Normal range: 1.5-2.0 L/min
   - Critical: <0.5 L/min (blockage)

## Common Anomalies

### 1. Valve Blockage
**Symptoms**:
- P1 increases (pressure builds up)
- F1 decreases (flow restricted)
- TE1 increases (friction heat)

**Root Causes**:
- Scale buildup (70% of cases)
- Foreign object (20%)
- Valve seat damage (10%)

**Actions**:
1. Schedule valve inspection
2. Check maintenance log (last cleaning?)
3. Prepare backup valve

### 2. Cooling System Failure
**Symptoms**:
- TE1, TE2 both increase
- P1, P2, F1 remain normal

**Root Causes**:
- Coolant pump failure
- Coolant leak
- Heat exchanger fouling

**Actions**:
1. Check coolant level
2. Inspect pump operation
3. Emergency shutdown if >90Â°C

### 3. Sensor Drift
**Symptoms**:
- Only 1 sensor shows abnormal value
- Other sensors normal
- Value changes slowly, not suddenly

**Actions**:
1. Cross-check with manual gauge
2. Review calibration records
3. Schedule sensor replacement if needed
```

**êµ¬í˜„**:
```python
# experiments/knowledge_base_manager.py

def load_domain_knowledge(dataset):
    """Load domain knowledge for dataset"""
    knowledge_files = {
        "SKAB": "knowledge_base/SKAB_valve_system.md",
        "SMD": "knowledge_base/SMD_server_metrics.md"
    }

    with open(knowledge_files[dataset]) as f:
        return f.read()

def retrieve_relevant_knowledge(query, knowledge_base):
    """RAG: Retrieve relevant sections from knowledge base"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    # Split knowledge base into sections
    sections = knowledge_base.split("##")

    # TF-IDF similarity
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform([query] + sections)

    similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()

    # Return top 3 relevant sections
    top_indices = similarities.argsort()[-3:][::-1]
    return [sections[i] for i in top_indices]
```

### 2.4 í†µí•© ì„¤ëª… ì‹œìŠ¤í…œ

**ìµœì¢… êµ¬í˜„**:
```python
# experiments/integrated_explainer.py

class IntegratedExplainer:
    def __init__(self, openai_api_key):
        self.api_key = openai_api_key
        openai.api_key = self.api_key

    def explain_full(self, anomaly, model, domain_knowledge):
        """3-in-1 explanation: Anomaly + Model + Domain"""

        # 1. Anomaly explanation
        anomaly_exp = self.explain_anomaly(anomaly, domain_knowledge)

        # 2. Model explanation (SHAP + feature importance)
        model_exp = self.explain_model_decision(
            model=model,
            anomaly_point=anomaly['features'],
            feature_names=anomaly['feature_names']
        )

        # 3. Domain-connected explanation (RAG)
        domain_exp = self.explain_domain_context(
            anomaly=anomaly,
            model_explanation=model_exp,
            knowledge_base=domain_knowledge
        )

        # LLM synthesizes all 3
        return self.synthesize_explanation(
            anomaly_exp, model_exp, domain_exp
        )

    def synthesize_explanation(self, anomaly_exp, model_exp, domain_exp):
        """LLM combines 3 explanations into coherent narrative"""

        prompt = f"""
Synthesize a comprehensive explanation from:

1. Anomaly Analysis:
{anomaly_exp}

2. Model Decision Analysis:
{model_exp}

3. Domain Context:
{domain_exp}

Create a unified explanation that:
- Tells a coherent story
- Links model decision to domain meaning
- Provides actionable insights
- Uses clear language for operators

Format:
## Executive Summary (2 sentences)
## What Happened (data-driven)
## Why The Model Flagged It (model-driven)
## Manufacturing Perspective (domain-driven)
## Root Cause Analysis (synthesized)
## Recommended Actions (prioritized)
"""

        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert integrating ML and manufacturing knowledge."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )

        return response.choices[0].message.content
```

### 2.5 Phase 2 ì‘ì—… ëª©ë¡ (Week 1-2)

**Week 1: ê¸°ë³¸ êµ¬í˜„**
- [ ] Day 1: OpenAI API í†µí•©, ê¸°ë³¸ prompt template
- [ ] Day 2: Anomaly explanation êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ (5ê°œ ìƒ˜í”Œ)
- [ ] Day 3: SHAP/Feature importance ì¶”ì¶œ ì½”ë“œ
- [ ] Day 4: ML ëª¨ë¸ í•´ì„ LLM prompt
- [ ] Day 5: SKAB ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ ì‘ì„±
- [ ] Day 6: SMD ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ ì‘ì„±
- [ ] Day 7: RAG retrieval êµ¬í˜„

**Week 2: í†µí•© ë° ê²€ì¦**
- [ ] Day 8: 3-in-1 í†µí•© ì„¤ëª… ì‹œìŠ¤í…œ
- [ ] Day 9: 10ê°œ anomaly ìƒ˜í”Œ ì„¤ëª… ìƒì„±
- [ ] Day 10: ì„¤ëª… í’ˆì§ˆ í‰ê°€ (ì‚¬ëŒ or GPT-4 judge)
- [ ] Day 11: ë¬¸ì œì  ìˆ˜ì • ë° ê°œì„ 
- [ ] Day 12-14: Phase 2 ë³´ê³ ì„œ ì‘ì„±

**ì„±ê³¼ë¬¼**:
- `experiments/llm_explainer.py` - í†µí•© ì„¤ëª… ì‹œìŠ¤í…œ
- `experiments/knowledge_base/` - ë„ë©”ì¸ ì§€ì‹ (SKAB, SMD)
- `PHASE2_LLM_EXPLANATION_REPORT.md` - ì„¤ëª… ì˜ˆì‹œ 10ê°œ í¬í•¨
- Demo ìŠ¤í¬ë¦½íŠ¸: `scripts/demo_explanation.py`

---

## ğŸš€ Phase 3: LLM ê¸°ë°˜ ë„ë©”ì¸ ìµœì í™” (Week 3-4)

### ëª©í‘œ
LLMì´ ì œì¡° ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ML íŒŒë¼ë¯¸í„°ë¥¼ ì œì•ˆ â†’ ì„±ëŠ¥ ê°œì„ 

### 3.1 LLM Parameter Advisor

**ì‹œë‚˜ë¦¬ì˜¤**:
```
User: "SKAB ë°ì´í„°ì—ì„œ IsolationForest F1ì´ 0.033ìœ¼ë¡œ ë‚®ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ê°œì„ í•˜ë‚˜ìš”?"

LLM: "SKAB ë°¸ë¸Œ ì‹œìŠ¤í…œì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ë©´:
1. window_size: 50 â†’ 150
   - ì´ìœ : ë°¸ë¸Œ ì´ìƒì€ 50-200 timesteps ì§€ì†
2. contamination: 0.1 â†’ 0.35
   - ì´ìœ : Anomaly rateê°€ 35%
3. n_estimators: 100 â†’ 200
   - ì´ìœ : 8ê°œ ì„¼ì„œë¡œ feature space ë³µì¡

ì˜ˆìƒ ê°œì„ : F1 0.033 â†’ 0.15+"
```

**êµ¬í˜„**:
```python
# experiments/llm_parameter_advisor.py

def suggest_parameters(dataset, detector, current_performance, domain_knowledge):
    """LLM suggests optimal parameters based on domain knowledge"""

    prompt = f"""
You are an ML expert specializing in manufacturing anomaly detection.

Dataset: {dataset}
Domain characteristics:
{domain_knowledge}

Current setup:
- Detector: {detector}
- Performance: {current_performance}
- Parameters: {current_parameters}

Based on domain knowledge, suggest optimal parameters.

For each parameter:
1. Current value
2. Suggested value
3. Reason (domain-driven)
4. Expected impact

Also estimate expected performance improvement.
"""

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an ML parameter tuning expert."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    # Parse LLM response to extract parameters
    suggestions = parse_parameter_suggestions(response.choices[0].message.content)
    return suggestions
```

### 3.2 ì‹¤í—˜ í”„ë¡œí† ì½œ

**Baseline vs LLM-guided ë¹„êµ**:

```python
# experiments/llm_optimization_experiment.py

def run_optimization_experiment(dataset, detector):
    """Compare Baseline vs LLM-guided parameters"""

    # Baseline (default parameters)
    baseline_params = get_default_params(detector)
    baseline_results = run_experiments(
        dataset=dataset,
        detector=detector,
        params=baseline_params,
        seeds=range(42, 1042, 50)  # 20 seeds
    )

    # LLM-guided
    domain_knowledge = load_domain_knowledge(dataset)
    llm_params = suggest_parameters(
        dataset=dataset,
        detector=detector,
        current_performance=baseline_results['mean_f1'],
        domain_knowledge=domain_knowledge
    )

    llm_results = run_experiments(
        dataset=dataset,
        detector=detector,
        params=llm_params,
        seeds=range(42, 1042, 50)  # Same 20 seeds
    )

    # Statistical comparison
    improvement = compare_results(baseline_results, llm_results)

    return {
        "baseline": baseline_results,
        "llm_guided": llm_results,
        "improvement": improvement,
        "llm_suggestions": llm_params
    }
```

### 3.3 Phase 3 ì‘ì—… ëª©ë¡ (Week 3-4)

**Week 3: LLM Parameter Advisor**
- [ ] Day 15: Parameter advisor prompt ì„¤ê³„
- [ ] Day 16: 3ê°œ detector Ã— 3ê°œ dataset = 9ê°€ì§€ ì œì•ˆ ìƒì„±
- [ ] Day 17: ì œì•ˆ íŒŒë¼ë¯¸í„° ê²€ì¦ (í•©ë¦¬ì„± ì²´í¬)
- [ ] Day 18-20: LLM-guided íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹¤í—˜ (20 seeds each)
- [ ] Day 21: Baseline vs LLM-guided í†µê³„ ë¹„êµ

**Week 4: í‰ê°€ ë° ë³´ê³ ì„œ**
- [ ] Day 22: ì„±ëŠ¥ ê°œì„  ë¶„ì„ (Wilcoxon test)
- [ ] Day 23: ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„ (ì™œ ê°œì„  ì•ˆ ëëŠ”ê°€?)
- [ ] Day 24-26: Phase 3 ë³´ê³ ì„œ ì‘ì„±
- [ ] Day 27-28: ì „ì²´ í†µí•© ë° Demo

**ì„±ê³µ ê¸°ì¤€**:
- LLM-guided paramsê°€ baselineë³´ë‹¤ **í‰ê·  10% ì´ìƒ ê°œì„ ** (F1 ë˜ëŠ” AUC-PR)
- 9ê°œ ì¼€ì´ìŠ¤ ì¤‘ **ìµœì†Œ 6ê°œì—ì„œ ê°œì„ ** (66% ì„±ê³µë¥ )
- ê°œì„ ì´ **í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸** (p<0.05)

---

## ğŸ“Š ìµœì¢… ì„±ê³¼ë¬¼ (Deliverables)

### Phase 1 (ì™„ë£Œ âœ…)
- [x] `COMPREHENSIVE_EXPERIMENT_REPORT.md` (35 pages)
- [x] `runs/all_results.csv` (353 runs)
- [x] `runs/statistical_tests.json`, `bootstrap_ci.json`, `correlation_analysis.json`

### Phase 2 (Week 1-2)
- [ ] `experiments/llm_explainer.py` - í†µí•© ì„¤ëª… ì‹œìŠ¤í…œ
- [ ] `experiments/knowledge_base/` - SKAB, SMD ë„ë©”ì¸ ì§€ì‹
- [ ] `PHASE2_LLM_EXPLANATION_REPORT.md` - 10ê°œ ì„¤ëª… ì˜ˆì‹œ
- [ ] Demo: `scripts/demo_explanation.py`

### Phase 3 (Week 3-4)
- [ ] `experiments/llm_parameter_advisor.py` - Parameter suggestion system
- [ ] `PHASE3_LLM_OPTIMIZATION_REPORT.md` - Baseline vs LLM-guided ë¹„êµ
- [ ] Demo: `scripts/demo_optimization.py`

### ìµœì¢… í†µí•©
- [ ] `FINAL_INTEGRATED_REPORT.md` - Phase 1-3 í†µí•© ë³´ê³ ì„œ
  - ML íƒì§€ ì„±ëŠ¥ (Phase 1)
  - LLM í•´ì„ ëŠ¥ë ¥ (Phase 2)
  - LLM ìµœì í™” íš¨ê³¼ (Phase 3)
  - ê²°ë¡ : "LLM-guided approachê°€ ì œì¡° anomaly detectionì„ X% ê°œì„ "

---

## âš ï¸ ê¸°ì¡´ TODO.mdì™€ì˜ ì°¨ì´ì 

### ê¸°ì¡´ TODO.md (í•™ìˆ  ì¤‘ì‹¬)
- RQ1: Frequency vs Time domain
- RQ2: Ensemble methods
- RQ3: Metric correlation
- RQ4: Cost sensitivity

### ìƒˆ TODO (ì‹¤ìš© ì¤‘ì‹¬)
- **í•µì‹¬**: ML + LLM í†µí•©
- **ì°¨ë³„ì **:
  1. ì´ìƒì¹˜ í•´ì„ (LLM)
  2. **ëª¨ë¸ í•´ì„ (LLM)** â† ìƒˆë¡œìš´!
  3. ë„ë©”ì¸ ìµœì í™” (LLM)

### ìš°ì„ ìˆœìœ„
1. **Phase 2-3 (LLM í†µí•©)** - ì—°êµ¬ì˜ í•µì‹¬!
2. Phase 1 ê°œì„  (SpecCNN ìˆ˜ì • ë“±) - ë¶€ì°¨ì 

---

## ğŸ“… íƒ€ì„ë¼ì¸ (4ì£¼)

| Week | Phase | ëª©í‘œ | ì„±ê³¼ë¬¼ |
|------|-------|------|--------|
| **1** | Phase 2.1 | ì´ìƒì¹˜ + ëª¨ë¸ í•´ì„ êµ¬í˜„ | llm_explainer.py, ì§€ì‹ ë² ì´ìŠ¤ |
| **2** | Phase 2.2 | í†µí•© ì„¤ëª… ì‹œìŠ¤í…œ, ê²€ì¦ | Phase 2 ë³´ê³ ì„œ + 10ê°œ ì˜ˆì‹œ |
| **3** | Phase 3.1 | LLM parameter advisor, ì¬ì‹¤í—˜ | llm_parameter_advisor.py |
| **4** | Phase 3.2 | ì„±ëŠ¥ ë¹„êµ, ìµœì¢… ë³´ê³ ì„œ | Final integrated report |

---

## âœ… ë‹¤ìŒ ì¦‰ì‹œ ì‘ì—… (Next Immediate Actions)

**ì˜¤ëŠ˜ (Day 1)**:
1. [ ] OpenAI API í†µí•© (`experiments/llm_config.py`)
2. [ ] ì²« ë²ˆì§¸ anomaly ì„¤ëª… ìƒì„± (1ê°œ ìƒ˜í”Œ)
3. [ ] SKAB ë„ë©”ì¸ ì§€ì‹ ì‘ì„± ì‹œì‘

**ë‚´ì¼ (Day 2)**:
1. [ ] Feature importance ì¶”ì¶œ ì½”ë“œ
2. [ ] SHAP values ê³„ì‚°
3. [ ] ML ëª¨ë¸ í•´ì„ prompt ì‘ì„±

**ëª¨ë ˆ (Day 3)**:
1. [ ] 3-in-1 í†µí•© ì„¤ëª… ì‹œìŠ¤í…œ
2. [ ] 5ê°œ anomaly ì„¤ëª… ìƒì„±
3. [ ] ì„¤ëª… í’ˆì§ˆ í‰ê°€

---

**ì—°êµ¬ í•µì‹¬ ì¬í™•ì¸**:
> ì œì¡° ê³µì • ë°ì´í„°ì—ì„œ MLë¡œ ì´ìƒ íƒì§€ + LLMì´ (1) ì´ìƒì¹˜, (2) ëª¨ë¸, (3) ë„ë©”ì¸ í•´ì„ + LLMì´ ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ML ìµœì í™”
